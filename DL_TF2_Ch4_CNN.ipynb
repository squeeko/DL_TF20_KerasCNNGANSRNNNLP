{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_TF2_Ch4_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNd40DI7gM1VTBybctbQX2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squeeko/DL_TF20_KerasCNNGANSRNNNLP/blob/in_progress/DL_TF2_Ch4_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5xz5K0YVoyW"
      },
      "source": [
        "# ConvNets in TensorFlow 2.x\n",
        "\n",
        "In Tensorflow 2.x if we want to add a convolutional layer with 32 parallel features and a filter size of 3X3, we write:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy11cyBPUJoO"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2a0ar6W2_O"
      },
      "source": [
        "# model = models.Sequential()\n",
        "# model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape = (28, 28, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WNd-8nrXQ8E"
      },
      "source": [
        "This means that we are applying a 3×3 convolution on 28×28 images with one input\n",
        "channel (or input filters) resulting in 32 output channels (or output filters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLgpvjsEaKlt"
      },
      "source": [
        "## Pooling layers\n",
        "\n",
        "Let's suppose that we want to summarize the output of a feature map. Again, we\n",
        "can use the spatial contiguity of the output produced from a single feature map\n",
        "and aggregate the values of a sub-matrix into one single output value synthetically\n",
        "describing the \"meaning\" associated with that physical region.\n",
        "\n",
        "## Max pooling\n",
        "One easy and common choice is the so-called max-pooling operator, which simply\n",
        "outputs the maximum activation as observed in the region. In Keras, if we want\n",
        "to define a max pooling layer of size 2×2, we write:\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "## Average pooling\n",
        "Another choice is average pooling, which simply aggregates a region into the average\n",
        "values of the activations observed in that region.\n",
        "Note that Keras implements a large number of pooling layers and a complete list is\n",
        "available online (https://keras.io/layers/pooling/). In short, all the pooling\n",
        "operations are nothing more than a summary operation on a given region."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO8A4stFacQ3"
      },
      "source": [
        "## ConvNets summary\n",
        "So far, we have described the basic concepts of ConvNets. CNNs apply convolution\n",
        "and pooling operations in 1 dimension for audio and text data along the time\n",
        "dimension, in two dimensions for images along the (height × width) dimensions\n",
        "and in three dimensions for videos along the (height × width × time) dimensions.\n",
        "For images, sliding the filter over an input volume produces a map that provides\n",
        "the responses of the filter for each spatial position.\n",
        "\n",
        "In other words, a CNN has multiple filters stacked together that learn to recognize\n",
        "specific visual features independently from the location in the image itself. Those\n",
        "visual features are simple in the initial layers of the network and become more\n",
        "and more sophisticated deeper in the network. Training of a CNN requires the\n",
        "identification of the right values for each filter so that an input, when passed through\n",
        "multiple layers, activates certain neurons of the last layer so that it will predict the\n",
        "correct values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUg1c1efdsAc"
      },
      "source": [
        "## An example of DCNN ‒ LeNet\n",
        "\n",
        "**\"layers.Convolution2D(20, (5, 5), activation='relu', input_shape=input_\n",
        "shape))\"**\n",
        "\n",
        "Yann LeCun, who very recently won the Turing Award, proposed [1] a family\n",
        "of convnets named LeNet trained for recognizing MNIST handwritten characters\n",
        "with robustness to simple geometric transformations and distortion. The core idea\n",
        "of LeNets is to have lower layers alternating convolution operations with maxpooling\n",
        "operations. The convolution operations are based on carefully chosen local\n",
        "receptive fields with shared weights for multiple feature maps. Then, higher levels\n",
        "are fully connected based on a traditional MLP with hidden layers and softmax as\n",
        "output layer.\n",
        "\n",
        "Where the first parameter is the number of output filters in the convolution, and\n",
        "the next tuple is the extension of each filter. An interesting optional parameter is\n",
        "padding. There are two options: padding='valid' means that the convolution is\n",
        "only computed where the input and the filter fully overlap and therefore the output\n",
        "is smaller than the input, while padding='same' means that we have an output\n",
        "which is the same size as the input, for which the area around the input is padded\n",
        "with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veZJ3mI7W3CS"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "\n",
        "# network and training\n",
        "\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = tf.keras.optimizers.Adam()\n",
        "VALIDATION_SPLIT = 0.95\n",
        "\n",
        "IMG_ROWS, IMG_COLS = 28, 28 #  input image dimensions\n",
        "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
        "NB_CLASSES = 10 # number of outputs = number of digits"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE14y9bzNvke"
      },
      "source": [
        "We have a first convolutional stage with rectified linear unit (ReLU) activations\n",
        "followed by a max pooling. Our net will learn 20 convolutional filters, each one of\n",
        "which with a size of 5×5. The output dimension is the same as the input shape, so it\n",
        "will be 28×28. Note that since Convolution2D is the first stage of our pipeline, we\n",
        "are also required to define its input_shape. The max pooling operation implements\n",
        "a sliding window that slides over the layer and takes the maximum of each region\n",
        "with a step of 2 pixels both vertically and horizontally:\n",
        "\n",
        "Then there is a second convolutional stage with ReLU activations, followed again\n",
        "by a max pooling layer. In this case we increase the number of convolutional filters\n",
        "learned to 50 from the previous 20. Increasing the number of filters in deeper layers\n",
        "is a common technique used in deep learning:\n",
        "\n",
        "Then we have a pretty standard flattening and a dense network of 500 neurons,\n",
        "followed by a softmax classifier with 10 classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9OCcMJfW3FL"
      },
      "source": [
        "# define the LeNet ConvNet\n",
        "\n",
        "def build(input_shape, classes):\n",
        "  model = models.Sequential()\n",
        "  \n",
        "  # CONV => RELU => POOL\n",
        "  model.add(layers.Convolution2D(20, (5,5), activation='relu', input_shape=INPUT_SHAPE))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "  # CONV => RELU => POOL\n",
        "  model.add(layers.Convolution2D(50, (5,5), activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "  # Flatten => RELU layers\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(500, activation='relu'))\n",
        "\n",
        "  # Add a SoftMax classifier\n",
        "  model.add(layers.Dense(classes, activation=\"softmax\"))\n",
        "  return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR7v2viDouq2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrs0O3cjrspG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}