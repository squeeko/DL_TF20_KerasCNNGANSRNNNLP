{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_TF2_Ch1_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMX9XtkrmH8QSa9Wmksdj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squeeko/DL_TF20_KerasCNNGANSRNNNLP/blob/in_progress/DL_TF2_Ch1_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIWaE2V16yTU"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9LsBuBn7Sv9"
      },
      "source": [
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGgCHSuB8lxk"
      },
      "source": [
        "def load_data():\n",
        "  # Load data\n",
        "  (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "  # Pad sequences with max_len\n",
        "  X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "  X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "  return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhoMmjEo_k3F"
      },
      "source": [
        "### Tensorflow Layer Types\n",
        "\n",
        "[Embedding Layer - ML Mastery](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        ", [Embedding Layer - Tensorflow](https://keras.io/api/layers/core_layers/embedding/#embedding)\n",
        "\n",
        "[Max, Average, Global Max, Global Average](https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/)\n",
        "\n",
        "[Dense and Dropout](https://www.quora.com/In-TensorFlow-what-is-a-dense-and-a-dropout-layer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVnSMgZt9mWU"
      },
      "source": [
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # Input - Embedding Layer\n",
        "  # The model will take as input an integer matrix of size (batch, input_length)\n",
        "  # The model will output dimension (input_length, dim_embedding)\n",
        "  # The largest integer in the input should be no larger that n_words (vocabulary_size)\n",
        "  model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  # Takes the maximum value of either feature vector from each of the n_words features\n",
        "  model.add(layers.GlobalMaxPool1D())\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "  return(model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ3KDKDDFWL-",
        "outputId": "df0c4cbd-3819-41b9-d7f5-0cccdcd0876a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "score = model.fit(X_train, y_train,\n",
        "                  epochs = EPOCHS,\n",
        "                  batch_size = BATCH_SIZE,\n",
        "                  validation_data = (X_test, y_test)\n",
        "                  )\n",
        "\n",
        "score = model.evaluate(X_test, y_test, batch_size = BATCH_SIZE)\n",
        "\n",
        "print(\"\\nTest score: \", score[0])\n",
        "print(\"\\nTest accuracy: \", score[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.6714 - accuracy: 0.6434 - val_loss: 0.6294 - val_accuracy: 0.8318\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4599 - accuracy: 0.8400 - val_loss: 0.3697 - val_accuracy: 0.8521\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.2813 - accuracy: 0.8888 - val_loss: 0.3102 - val_accuracy: 0.8724\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.2166 - accuracy: 0.9159 - val_loss: 0.2933 - val_accuracy: 0.8763\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.1696 - accuracy: 0.9401 - val_loss: 0.2902 - val_accuracy: 0.8769\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 5s 98ms/step - loss: 0.1313 - accuracy: 0.9551 - val_loss: 0.2960 - val_accuracy: 0.8729\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.1010 - accuracy: 0.9684 - val_loss: 0.3049 - val_accuracy: 0.8714\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0762 - accuracy: 0.9775 - val_loss: 0.3206 - val_accuracy: 0.8669\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0570 - accuracy: 0.9844 - val_loss: 0.3455 - val_accuracy: 0.8604\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.0441 - accuracy: 0.9893 - val_loss: 0.3534 - val_accuracy: 0.8595\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 0.0322 - accuracy: 0.9926 - val_loss: 0.3744 - val_accuracy: 0.8573\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0255 - accuracy: 0.9946 - val_loss: 0.3882 - val_accuracy: 0.8553\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.0205 - accuracy: 0.9960 - val_loss: 0.4091 - val_accuracy: 0.8530\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 5s 96ms/step - loss: 0.0158 - accuracy: 0.9973 - val_loss: 0.4237 - val_accuracy: 0.8526\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.0131 - accuracy: 0.9979 - val_loss: 0.4387 - val_accuracy: 0.8530\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 5s 97ms/step - loss: 0.0108 - accuracy: 0.9984 - val_loss: 0.4483 - val_accuracy: 0.8530\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 0.0088 - accuracy: 0.9988 - val_loss: 0.4627 - val_accuracy: 0.8530\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.4744 - val_accuracy: 0.8517\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.4909 - val_accuracy: 0.8488\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.5018 - val_accuracy: 0.8498\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.5018 - accuracy: 0.8498\n",
            "\n",
            "Test score:  0.5017884373664856\n",
            "\n",
            "Test accuracy:  0.8497999906539917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcGUEY-gNOEx"
      },
      "source": [
        "## A practical overview of backpropagation\n",
        "\n",
        "Multi-layer perceptrons learn from training data through a process called\n",
        "backpropagation. In this section, we will cover the basics while more details can be\n",
        "found in Chapter 15, The Math behind Deep Learning. The process can be described as a\n",
        "way of progressively correcting mistakes as soon as they are detected. Let's see how\n",
        "this works.\n",
        "Remember that each neural network layer has an associated set of weights that\n",
        "determine the output values for a given set of inputs. Additionally, remember that\n",
        "a neural network can have multiple hidden layers.\n",
        "At the beginning, all the weights have some random assignment. Then, the net is\n",
        "activated for each input in the training set: values are propagated forward from the\n",
        "input stage through the hidden stages to the output stage where a prediction is\n",
        "made. Note that we've kept Figure 38 simple by only representing a few values with\n",
        "green dotted lines but in reality all the values are propagated forward through the\n",
        "network:\n",
        "\n",
        "![](https://raw.githubusercontent.com/squeeko/DL_TF20_KerasCNNGANSRNNNLP/in_progress/images/FWD_Step_in_BackProp.png)\n",
        "\n",
        "Since we know the true observed value in the training set, it is possible to calculate\n",
        "the error made in prediction. The key intuition for backtracking is to propagate the\n",
        "error back (see Figure 39), using an appropriate optimizer algorithm such as gradient\n",
        "descent to adjust the neural network weights with the goal of reducing the error\n",
        "(again, for the sake of simplicity, only a few error values are represented here):\n",
        "\n",
        "![](https://raw.githubusercontent.com/squeeko/DL_TF20_KerasCNNGANSRNNNLP/in_progress/images/BWD_STEP_in_BackProp.png)\n",
        "\n",
        "The process of forward propagation from input to output and the backward\n",
        "propagation of errors is repeated several times until the error gets below a\n",
        "predefined threshold. The whole process is represented in Figure 40:\n",
        "\n",
        "![](https://raw.githubusercontent.com/squeeko/DL_TF20_KerasCNNGANSRNNNLP/in_progress/images/FWD_and_BWD_Prop.png)\n",
        "\n",
        "The features represent the input, and the labels are used here to drive the learning\n",
        "process. The model is updated in such a way that the loss function is progressively\n",
        "minimized. In a neural network, what really matters is not the output of a single\n",
        "neuron but the collective weights adjusted in each layer. Therefore, the network\n",
        "progressively adjusts its internal weights in such a way that the prediction increases\n",
        "the number of correctly forecasted labels. Of course, using the right set of features\n",
        "and having quality labeled data is fundamental in order to minimize the bias during\n",
        "the learning process."
      ]
    }
  ]
}