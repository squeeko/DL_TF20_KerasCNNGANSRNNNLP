{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_TF2_Ch1_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2IqPBuqjLehaEKQuBmDF+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squeeko/DL_TF20_KerasCNNGANSRNNNLP/blob/in_progress/DL_TF2_Ch1_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIWaE2V16yTU"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9LsBuBn7Sv9"
      },
      "source": [
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGgCHSuB8lxk"
      },
      "source": [
        "def load_data():\n",
        "  # Load data\n",
        "  (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "  # Pad sequences with max_len\n",
        "  X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "  X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "  return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhoMmjEo_k3F"
      },
      "source": [
        "### Tensorflow Layer Types\n",
        "\n",
        "[Embedding Layer - ML Mastery](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        ", [Embedding Layer - Tensorflow](https://keras.io/api/layers/core_layers/embedding/#embedding)\n",
        "\n",
        "[Max, Average, Global Max, Global Average](https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVnSMgZt9mWU"
      },
      "source": [
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # Input - Embedding Layer\n",
        "  # The model will take as input an integer matrix of size (batch, input_length)\n",
        "  # The model will output dimension (input_length, dim_embedding)\n",
        "  # The largest integer in the input should be no larger that n_words (vocabulary_size)\n",
        "    model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "  # Takes the maximum value of either feature vector from each of the n_words features\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}